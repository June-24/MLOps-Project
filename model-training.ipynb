{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e74fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report, matthews_corrcoef)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test the model\n",
    "def train_model(X_train, y_train, X_test, y_test, feature_names):\n",
    "    # Initialize and fit the model\n",
    "    done_model = RandomForestClassifier(\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        n_estimators=15, #try 50 # was 100\n",
    "        #max_depth = 10, #default is none\n",
    "        #max_features=3, # Fewer features per split, less memory\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    done_model.fit(X_train, y_train)\n",
    "\n",
    "    # Test predictions\n",
    "    y_pred = done_model.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    metrics_rf = calculate_performance_metrics(y_test, y_pred)\n",
    "    print_performance_metrics(metrics_rf)\n",
    "    feature_importance(done_model, X_train, feature_names)\n",
    "\n",
    "    return done_model\n",
    "\n",
    "# Useful values for classification\n",
    "def calculate_performance_metrics(y_test, y_pred):\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_test, y_pred)\n",
    "    metrics['precision'] = precision_score(y_test, y_pred, average='weighted')\n",
    "    metrics['recall'] = recall_score(y_test, y_pred, average='weighted')\n",
    "    metrics['f1_score'] = f1_score(y_test, y_pred, average='weighted')\n",
    "    metrics['confusion_matrix'] = confusion_matrix(y_test, y_pred)\n",
    "    metrics['mcc'] = matthews_corrcoef(y_test, y_pred)\n",
    "    metrics['classification_report'] = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Prints all performance metrics\n",
    "def print_performance_metrics(metrics):\n",
    "    print(\"Accuracy:\", metrics.get('accuracy', \"Not computed\"))\n",
    "    print(\"Precision:\", metrics.get('precision', \"Not computed\"))\n",
    "    print(\"Recall:\", metrics.get('recall', \"Not computed\"))\n",
    "    print(\"F1 Score:\", metrics.get('f1_score', \"Not computed\"))\n",
    "    print(\"Confusion Matrix:\\n\", metrics.get('confusion_matrix', \"Not computed\"))\n",
    "    print(\"Matthews Correlation Coefficient (MCC):\", metrics.get('mcc', \"Not computed\"))\n",
    "    print(\"Classification Report:\\n\", metrics.get('classification_report', \"Not computed\"))\n",
    "\n",
    "# Determine the feature importance in the model\n",
    "def feature_importance(model, X, feature_names):\n",
    "    feature_importances = model.feature_importances_\n",
    "    feature_importances_list = [(feature_names[j], importance) for j, importance in enumerate(feature_importances)]\n",
    "    feature_importances_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"Feature Importances:\")\n",
    "    for feature, importance in feature_importances_list:\n",
    "        print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b37816b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model\n",
    "def create_model():\n",
    "    # Load the data\n",
    "    data_name = ''\n",
    "    processed_data = pd.read_csv(data_name)\n",
    "\n",
    "    # Separate data\n",
    "    target_name = ''\n",
    "    X = processed_data.drop(columns=[target_name]).values\n",
    "    y = processed_data[target_name].values\n",
    "\n",
    "    feature_names = processed_data.columns[:-1].tolist()\n",
    "\n",
    "    # Get test and train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print('Read the data')\n",
    "    # Run the model\n",
    "    model = train_model(X_train, y_train, X_test, y_test, feature_names)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ded18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the model\n",
    "def run_model_training():\n",
    "    # Train the model\n",
    "    create_model()\n",
    "\n",
    "run_model_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
